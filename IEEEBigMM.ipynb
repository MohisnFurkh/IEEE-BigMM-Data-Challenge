{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Activation, Dropout, Dense\n",
    "from keras.layers import Flatten, LSTM\n",
    "from keras.layers import GlobalMaxPooling1D\n",
    "from keras.models import Model\n",
    "from keras.layers.embeddings import Embedding\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from scipy.sparse import hstack\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import punkt\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7978, 11)\n"
     ]
    }
   ],
   "source": [
    "def load_MeToo_data(path='MeTooMMD_train.csv'):\n",
    "\treturn pd.read_csv(path)\n",
    "MeToo=load_MeToo_data()\n",
    "\n",
    "print(MeToo.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6867, 11)\n",
      "(6867, 2)\n"
     ]
    }
   ],
   "source": [
    "comments=pd.read_csv('Tweet_cmt.csv')\n",
    "MeToo = MeToo[MeToo['TweetId'].isin(comments['Id'])]\n",
    "\n",
    "print(MeToo.shape)\n",
    "print(comments.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "MeToo_labels = MeToo[[\"Text_Only_Informative\", \"Image_Only_Informative\", \"Directed_Hate\",\\\n",
    "                         \"Generalized_Hate\", \"Sarcasm\", \"Allegation\", \"Justification\", \"Refutation\", \\\n",
    "                        \"Support\", \"Oppose\"]]\n",
    "MeToo_labels.shape\n",
    "\n",
    "class_names = [\"Text_Only_Informative\", \"Image_Only_Informative\", \"Directed_Hate\",\\\n",
    "                         \"Generalized_Hate\", \"Sarcasm\", \"Allegation\", \"Justification\", \"Refutation\", \\\n",
    "                        \"Support\", \"Oppose\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(sen):\n",
    "    # Remove punctuations and numbers\n",
    "    sentence = re.sub('[^a-zA-Z]', ' ', sen)\n",
    "\n",
    "    # Single character removal\n",
    "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
    "\n",
    "    # Removing multiple spaces\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
    "\n",
    "    return sentence\n",
    "X = []\n",
    "sentences = list(comments[\"Comments\"])\n",
    "for sen in sentences:\n",
    "    X.append(preprocess_text(sen))\n",
    "\n",
    "y = MeToo_labels.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_output=pd.read_csv('target.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LogisticRegression Classifier for Text_Only_Informative is complete!!\n",
      "Training LogisticRegression Classifier for Image_Only_Informative is complete!!\n",
      "Training LogisticRegression Classifier for Directed_Hate is complete!!\n",
      "Training LogisticRegression Classifier for Generalized_Hate is complete!!\n",
      "Training LogisticRegression Classifier for Sarcasm is complete!!\n",
      "Training LogisticRegression Classifier for Allegation is complete!!\n",
      "Training LogisticRegression Classifier for Justification is complete!!\n",
      "Training LogisticRegression Classifier for Refutation is complete!!\n",
      "Training LogisticRegression Classifier for Support is complete!!\n",
      "Training LogisticRegression Classifier for Oppose is complete!!\n"
     ]
    }
   ],
   "source": [
    "from zeugma.embeddings import EmbeddingTransformer\n",
    "\n",
    "submission=pd.DataFrame(MeToo, columns = [[\"Text_Only_Informative\", \"Image_Only_Informative\", \"Directed_Hate\",\\\n",
    "                         \"Generalized_Hate\", \"Sarcasm\", \"Allegation\", \"Justification\", \"Refutation\", \\\n",
    "                        \"Support\", \"Oppose\"]])\n",
    "glove = EmbeddingTransformer('glove')\n",
    "X_train = glove.transform(X_train)\n",
    "for class_name in class_names:\n",
    "    train_target=target_output[class_name]\n",
    "    model = LogisticRegression(solver='sag')\n",
    "    model.fit(X_train, train_target)\n",
    "    print('Training LogisticRegression Classifier for {} is complete!!'.format(class_name))\n",
    "X_test = glove.transform(X_test)\n",
    "sumbission=model.predict(X_test)\n",
    "submission.to_csv('submission_LogisticRegression.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400000it [00:45, 8708.44it/s]\n",
      "  1%|▏         | 74/5493 [00:00<08:10, 11.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5493/5493 [00:06<00:00, 788.39it/s]\n",
      "100%|██████████| 1374/1374 [00:01<00:00, 809.06it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open('/media/mohsin/New Volume/Downloads/glove.6B/glove.6B.300d.txt', encoding=\"utf8\")\n",
    "for line in tqdm(f):\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    try:\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    except ValueError:\n",
    "        pass\n",
    "f.close()\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "# this function creates a normalized vector for the whole sentence\n",
    "def sent2vec(s):\n",
    "    words = str(s).lower()\n",
    "    words = word_tokenize(words)\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    words = [w for w in words if w.isalpha()]\n",
    "    M = []\n",
    "    for w in words:\n",
    "        try:\n",
    "            M.append(embeddings_index[w])\n",
    "        except:\n",
    "            continue\n",
    "    M = np.array(M)\n",
    "    v = M.sum(axis=0)\n",
    "    if type(v) != np.ndarray:\n",
    "        return np.zeros(300)\n",
    "    return v / np.sqrt((v ** 2).sum())\n",
    "\n",
    "# create sentence vectors using the above function for training and validation set\n",
    "xtrain_glove = [sent2vec(x) for x in tqdm(X_train)]\n",
    "xtest_glove = [sent2vec(x) for x in tqdm(X_test)]\n",
    "\n",
    "#print('Checkpoint2 -Normalized Vector for Sentences are created')\n",
    "\n",
    "xtrain_glove = np.array(xtrain_glove)\n",
    "xtest_glove = np.array(xtest_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "#submission = pd.DataFrame.from_dict({'id': test['id']})\n",
    "\n",
    "submission=pd.DataFrame(target_output, columns = [[\"Text_Only_Informative\", \"Image_Only_Informative\", \"Directed_Hate\",\\\n",
    "                         \"Generalized_Hate\", \"Sarcasm\", \"Allegation\", \"Justification\", \"Refutation\", \\\n",
    "                        \"Support\", \"Oppose\"]])\n",
    "\n",
    "for class_name in class_names:\n",
    "    \n",
    "    train_target=target_output[class_name]\n",
    "    classifier = LogisticRegression(solver='sag')\n",
    "    classifier.fit(xtrain_glove, train_target)\n",
    "    print('Training LogisticRegression Classifier for {} is complete!!'.format(class_name))\n",
    "    \n",
    "    submission = classifier.predict_proba(xtest_glove)\n",
    "\n",
    "print('Total CV score is {}'.format(np.mean(scores)))\n",
    "\n",
    "#submission.to_csv('submission_LogisticRegression.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LogisticRegression Classifier for Text_Only_Informative is complete!!\n",
      "Training LogisticRegression Classifier for Image_Only_Informative is complete!!\n",
      "Training LogisticRegression Classifier for Directed_Hate is complete!!\n",
      "Training LogisticRegression Classifier for Generalized_Hate is complete!!\n",
      "Training LogisticRegression Classifier for Sarcasm is complete!!\n",
      "Training LogisticRegression Classifier for Allegation is complete!!\n",
      "Training LogisticRegression Classifier for Justification is complete!!\n",
      "Training LogisticRegression Classifier for Refutation is complete!!\n",
      "Training LogisticRegression Classifier for Support is complete!!\n",
      "Training LogisticRegression Classifier for Oppose is complete!!\n"
     ]
    }
   ],
   "source": [
    "submission=pd.DataFrame(MeToo, columns = [[\"Text_Only_Informative\", \"Image_Only_Informative\", \"Directed_Hate\",\\\n",
    "                         \"Generalized_Hate\", \"Sarcasm\", \"Allegation\", \"Justification\", \"Refutation\", \\\n",
    "                        \"Support\", \"Oppose\"]])\n",
    "\n",
    "for class_name in class_names:\n",
    "    train_target=target_output[class_name]\n",
    "    classifier = LogisticRegression(solver='sag')\n",
    "    classifier.fit(xtrain_glove, train_target)\n",
    "    print('Training LogisticRegression Classifier for {} is complete!!'.format(class_name))\n",
    "    \n",
    "submission = classifier.predict_proba(xtest_glove)\n",
    "\n",
    "#submission.to_csv('submission_LogisticRegression.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
